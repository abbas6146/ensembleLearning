{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZPIfFK0znAd"
      },
      "source": [
        "## Downloading source code from the github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q99-M52MhE6Z",
        "outputId": "528a4e24-04d5-4594-e36d-5aa0f2fd0e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ensembleLearning'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 133 (delta 60), reused 90 (delta 34), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (133/133), 331.27 KiB | 1.76 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shivaditya-meduri/ensembleLearning.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoaokwXpzLIe"
      },
      "source": [
        "## Decision Tree Classifier - Implemented from scratch\n",
        "Using the gini-impurity cost function to split the data by feature and rank them based on importance, we created a Decision Tree classifier from scratch. We included 2 hyper-parameters which are max depth of the tree and the minimum samples count per leaf to tune the model. Below, we tested the model on 2 datasets which are the breast cancer dataset and the iris dataset for the task of classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36uf-PbNzUMk"
      },
      "source": [
        "#### Testing on Breast Cancer dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XZzbjCZhMXw",
        "outputId": "c065b10e-a56c-4770-c8bd-a014d95ceed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the breast cancer dataset is  0.9298245614035088\n"
          ]
        }
      ],
      "source": [
        "## Testing on breast cancer dataset which predcits if a sample to \"Benign\" or \"Malignant\" case of cancer\n",
        "import pandas as pd\n",
        "from ensembleLearning.src.decisionTree import decisionTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = pd.read_csv(\"ensembleLearning/data/bcan.csv\")\n",
        "X = data.drop([\"diagnosis\", \"id\"], axis=1).values\n",
        "y = data[\"diagnosis\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "dt = decisionTree(max_depth=50, min_samples_leaf=1)\n",
        "dt.train(X_train, y_train)\n",
        "ypred = dt.predict(X_test)\n",
        "print(\"Accuracy on the breast cancer dataset is \", accuracy_score(ypred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-Parameter tuning for breast cancer dataset:"
      ],
      "metadata": {
        "id": "XSP6XC0AaErN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.decisionTree import decisionTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = pd.read_csv(\"ensembleLearning/data/bcan.csv\")\n",
        "X = data.drop([\"diagnosis\", \"id\"], axis=1).values\n",
        "y = data[\"diagnosis\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "# hyper params\n",
        "max_depth = [30, 40, 50, 60]\n",
        "min_samples_leaf = [1, 2, 3, 4]\n",
        "threshold = [0.01, 0.03, 0.1]\n",
        "results = {}\n",
        "for depth in max_depth:\n",
        "    for sample in min_samples_leaf:\n",
        "        for thresh in threshold:          \n",
        "          dt = decisionTree(max_depth=depth, min_samples_leaf=sample, threshold=thresh)\n",
        "          dt.train(X_train, y_train)\n",
        "          ypred = dt.predict(X_test)\n",
        "          results[(depth, sample, thresh)] = accuracy_score(ypred, y_test)\n",
        "\n",
        "accuracies = list(results.values())\n",
        "params = list(results.keys())\n",
        "best_params = params[accuracies.index(max(accuracies))]\n",
        "\n",
        "print(\"Best accuracy = {}. Model params: (max_depth={}, min_samples_leaf={}, threshold={})\".format(max(accuracies), best_params[0], best_params[1], best_params[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3tD3NwSaCRB",
        "outputId": "b467824f-5290-4365-b820-464acec15885"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracy = 0.9473684210526315. Model params: (max_depth=30, min_samples_leaf=1, threshold=0.01)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WtABUH5zZEw"
      },
      "source": [
        "#### Testing on Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taJSV7VUzDwS",
        "outputId": "c9c01576-58ff-45ef-bd42-c69495bcffb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on iris dataset is  1.0\n"
          ]
        }
      ],
      "source": [
        "## Testing on iris dataset which classifies flower physical charecteristics to type of flower : Setosa, Verginica and Versicolor\n",
        "import pandas as pd\n",
        "from ensembleLearning.src.decisionTree import decisionTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = pd.read_csv(\"ensembleLearning/data/iris.csv\")\n",
        "X = data.drop([\"variety\"], axis=1).values\n",
        "y = data[\"variety\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "dt = decisionTree(type=\"classification\", max_depth=100, min_samples_leaf=1)\n",
        "dt.train(X_train, y_train)\n",
        "ypred = dt.predict(X_test)\n",
        "print(\"Accuracy on iris dataset is \", accuracy_score(ypred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-Parameter tuning on iris dataset:"
      ],
      "metadata": {
        "id": "LjU4qL8chcAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.decisionTree import decisionTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = pd.read_csv(\"ensembleLearning/data/iris.csv\")\n",
        "X = data.drop([\"variety\"], axis=1).values\n",
        "y = data[\"variety\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "max_depth = [3, 10, 50, 100]\n",
        "min_samples_leaf = [1, 2, 3, 4]\n",
        "threshold = [0.01, 0.03, 0.1]\n",
        "results = {}\n",
        "for depth in max_depth:\n",
        "    for sample in min_samples_leaf:\n",
        "        for thresh in threshold:\n",
        "          dt = decisionTree(max_depth=depth, min_samples_leaf=sample, threshold=thresh)\n",
        "          dt.train(X_train, y_train)\n",
        "          ypred = dt.predict(X_test)\n",
        "          results[(depth, sample, thresh)] = accuracy_score(ypred, y_test)\n",
        "\n",
        "accuracies = list(results.values())\n",
        "params = list(results.keys())\n",
        "best_params = params[accuracies.index(max(accuracies))]\n",
        "\n",
        "print(\"Best accuracy = {}. Model params: (max_depth={}, min_samples_leaf={}, threshold={})\".format(max(accuracies), best_params[0], best_params[1], best_params[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4D2JFZuewn0",
        "outputId": "c952e289-a11c-49e3-ae12-46abc318eb20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracy = 0.9333333333333333. Model params: (max_depth=3, min_samples_leaf=4, threshold=0.01)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZrBTmRfz240"
      },
      "source": [
        "### Decision Tree Regressor\n",
        "\n",
        "Using variance as cost function to split the data by feature and rank them by importance, and then using the average of all the labels in the leaf node, we make a prediction for a given set of features after training through traversal of the binary tree created. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xtTxoRlzlKa",
        "outputId": "a2168b20-91e5-468f-9537-b21f854aeef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error of the model is :  53117.38009733844\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.decisionTree import decisionTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "reg_data = pd.read_csv(\"ensembleLearning/data/regression_housing.csv\")[[\"MSSubClass\", \"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"SalePrice\"]]\n",
        "X = reg_data.drop([\"SalePrice\"], axis=1).values\n",
        "y = reg_data[\"SalePrice\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "dt = decisionTree(type = \"regression\")\n",
        "dt.train(X_train, y_train)\n",
        "ypred = dt.predict(X_test)\n",
        "print(\"Root Mean Squared Error of the model is : \", math.sqrt(mean_squared_error(ypred, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-Parameter tuning for regression tree on housing dataset:"
      ],
      "metadata": {
        "id": "zOVzNiwdiU7g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8428519d-1d36-4f42-f9f6-62da45499045",
        "id": "povdyvZFg417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowest root mean squared error = 50450.84499744902. Model params: (max_depth=10, min_samples_leaf=1, threshold=0.01)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.decisionTree import decisionTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "reg_data = pd.read_csv(\"ensembleLearning/data/regression_housing.csv\")[[\"MSSubClass\", \"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"SalePrice\"]]\n",
        "X = reg_data.drop([\"SalePrice\"], axis=1).values\n",
        "y = reg_data[\"SalePrice\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "max_depth = [3, 10, 50, 100]\n",
        "min_samples_leaf = [1, 2, 3, 4]\n",
        "threshold = [0.01, 0.03, 0.1]\n",
        "results = {}\n",
        "for depth in max_depth:\n",
        "    for sample in min_samples_leaf:\n",
        "        for thresh in threshold:   \n",
        "          dt = decisionTree(type = \"regression\", max_depth=depth, min_samples_leaf=sample, threshold=thresh)\n",
        "          dt.train(X_train, y_train)\n",
        "          ypred = dt.predict(X_test)\n",
        "          results[(depth, sample, thresh)] = math.sqrt(mean_squared_error(ypred, y_test))\n",
        "\n",
        "\n",
        "errors = list(results.values())\n",
        "params = list(results.keys())\n",
        "best_params = params[errors.index(min(errors))]\n",
        "\n",
        "print(\"Lowest root mean squared error = {}. Model params: (max_depth={}, min_samples_leaf={}, threshold={})\".format(min(errors), best_params[0], best_params[1], best_params[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWHsNnhj0MCD"
      },
      "source": [
        "### Random Forest Classifier\n",
        "Using bagging method, we created an ensemble of Decision Tree classifiers and used a voting mechnaism to decide what the class of a set of features will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3e_Q6dU0CzU",
        "outputId": "2f9c0e83-d16c-4fc0-fc0d-0c958856196d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on iris dataset using a random forest model is  1.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.randomForest import randomForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = pd.read_csv(\"ensembleLearning/data/iris.csv\")\n",
        "X = data.drop([\"variety\"], axis=1).values\n",
        "y = data[\"variety\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "rf = randomForest(type = \"classification\", n_trees=50, max_depth=100, min_samples_leaf=1)\n",
        "rf.train(X_train, y_train)\n",
        "ypred = rf.predict(X_test)\n",
        "print(\"Accuracy on iris dataset using a random forest model is \", accuracy_score(ypred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-Parameter tuning for Random Forest on iris dataset:"
      ],
      "metadata": {
        "id": "XNaDSHstoMwl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6ec074-3abb-42d7-babf-af7751dac931",
        "id": "_VOf1BOlingC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracy = 0.9333333333333333. Model params: (n_trees=10, max_depth=3, min_samples_leaf=1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.randomForest import randomForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = pd.read_csv(\"ensembleLearning/data/iris.csv\")\n",
        "X = data.drop([\"variety\"], axis=1).values\n",
        "y = data[\"variety\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "n_trees = [10, 30, 50 ,100]\n",
        "max_depth = [3, 10, 50, 100]\n",
        "min_samples_leaf = [1, 2, 3, 4]\n",
        "\n",
        "results = {}\n",
        "for tree in n_trees:\n",
        "  for depth in max_depth:\n",
        "      for sample in min_samples_leaf:\n",
        "          rf = randomForest(type = \"classification\", n_trees=tree, max_depth=depth, min_samples_leaf=sample)\n",
        "          rf.train(X_train, y_train)\n",
        "          ypred = rf.predict(X_test)\n",
        "          results[(tree, depth, sample)] = accuracy_score(ypred, y_test)\n",
        "\n",
        "accuracies = list(results.values())\n",
        "params = list(results.keys())\n",
        "best_params = params[accuracies.index(max(accuracies))]\n",
        "\n",
        "print(\"Best accuracy = {}. Model params: (n_trees={}, max_depth={}, min_samples_leaf={})\".format(max(accuracies), best_params[0], best_params[1], best_params[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-59KRDD0bnX"
      },
      "source": [
        "### Random Forest Regressor\n",
        "Using Decision Tree Regressor base regression model, an ensemble method using bagging is created which is nothing but the Random Forest Regressor. Instead of voting mechanism like in the case of Random Forest Classifier, we take the average of all the predictions by all the base estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3EXEOB50fxL",
        "outputId": "e195703a-336b-4596-bb9a-562d2e933a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error of the model is :  52611.77811114424\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.randomForest import randomForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "reg_data = pd.read_csv(\"ensembleLearning/data/regression_housing.csv\")[[\"MSSubClass\", \"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"SalePrice\"]]\n",
        "X = reg_data.drop([\"SalePrice\"], axis=1).values\n",
        "y = reg_data[\"SalePrice\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "rf = randomForest(type = \"regression\", n_trees=50, max_depth=100, min_samples_leaf=1)\n",
        "rf.train(X_train, y_train)\n",
        "ypred = rf.predict(X_test)\n",
        "print(\"Root Mean Squared Error of the model is : \", math.sqrt(mean_squared_error(ypred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ensembleLearning.src.randomForest import randomForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "reg_data = pd.read_csv(\"ensembleLearning/data/regression_housing.csv\")[[\"MSSubClass\", \"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"SalePrice\"]]\n",
        "X = reg_data.drop([\"SalePrice\"], axis=1).values\n",
        "y = reg_data[\"SalePrice\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "n_trees = [10, 30, 50 ,100]\n",
        "max_depth = [3, 10, 50, 100]\n",
        "min_samples_leaf = [1, 2, 3, 4]\n",
        "\n",
        "results = {}\n",
        "for tree in n_trees:\n",
        "  for depth in max_depth:\n",
        "      for sample in min_samples_leaf:\n",
        "        rf = randomForest(type = \"regression\", n_trees=50, max_depth=100, min_samples_leaf=1)\n",
        "        rf.train(X_train, y_train)\n",
        "        ypred = rf.predict(X_test)\n",
        "        results[(tree, depth, sample)] = math.sqrt(mean_squared_error(ypred, y_test))\n",
        "\n",
        "\n",
        "errors = list(results.values())\n",
        "params = list(results.keys())\n",
        "best_params = params[errors.index(min(errors))]\n",
        "\n",
        "print(\"Lowest root mean squared error = {}. Model params: (n_trees={}, max_depth={}, min_samples_leaf={})\".format(min(errors), best_params[0], best_params[1], best_params[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfC74ebMoZfi",
        "outputId": "408d5653-14db-43b7-dc95-ad4e831429d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowest root mean squared error = 55173.19230148441. Model params: (n_trees=50, max_depth=3, min_samples_leaf=2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_evuVINUpjtD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}