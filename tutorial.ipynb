{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier - Implemented from scratch\n",
    "Using the gini-impurity cost function to split the data by feature and rank them based on importance, we created a Decision Tree classifier from scratch. We included 2 hyper-parameters which are max depth of the tree and the minimum samples count per leaf to tune the model. Below, we tested the model on 2 datasets which are the breast cancer dataset and the iris dataset for the task of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing on breast cancer dataset which predcits if a sample to \"Benign\" or \"Malgnant\" case of cancer\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"bcan.csv\")\n",
    "X = data.drop([\"diagnosis\", \"id\"], axis=1).values\n",
    "y = data[\"diagnosis\"].values\n",
    "from decisionTree import decisionTree\n",
    "dt = decisionTree(max_depth=50, min_samples_leaf=1)\n",
    "dt.train(X, y)\n",
    "ypred = []\n",
    "ypred = dt.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the breast cancer dataset is  0.9876977152899824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on the breast cancer dataset is \", accuracy_score(ypred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing on iris dataset which classifies flower physical charecteristics to type of flower : Setosa, Verginica and Versicolor\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"iris.csv\")\n",
    "X = data.drop([\"variety\"], axis=1).values\n",
    "y = data[\"variety\"].values\n",
    "from decisionTree import decisionTree\n",
    "dt = decisionTree(type=\"classification\", max_depth=100, min_samples_leaf=1)\n",
    "dt.train(X, y)\n",
    "ypred = []\n",
    "ypred = dt.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on iris dataset is  0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on iris dataset is \", accuracy_score(ypred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n",
    "\n",
    "Using variance as cost function to split the data by feature and rank them by importance, and then using the average of all the labels in the leaf node, we make a prediction for a given set of features after training through traversal of the binary tree created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reg_data = pd.read_csv(\"regression_housing.csv\")[[\"MSSubClass\", \"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"SalePrice\"]]\n",
    "X = reg_data.drop([\"SalePrice\"], axis=1).values\n",
    "y = reg_data[\"SalePrice\"].values\n",
    "from decisionTree import decisionTree\n",
    "dt = decisionTree(type = \"regression\")\n",
    "dt.train(X, y)\n",
    "ypred = [], []\n",
    "ypred = dt.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50070.42903592454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "print(math.sqrt(mean_squared_error(ypred, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "#### Using bagging method, we created an ensemble of Decision Tree classifiers and used a voting mechnaism to decide what the class of a set of features will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"iris.csv\")\n",
    "X = data.drop([\"variety\"], axis=1).values\n",
    "y = data[\"variety\"].values\n",
    "from randomForest import randomForest\n",
    "rf = randomForest(type = \"classification\", max_depth=100, min_samples_leaf=1)\n",
    "rf.train(X, y)\n",
    "ypred = rf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on iris dataset using a random forest model is  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on iris dataset using a random forest model is \", accuracy_score(ypred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "Using Decision Tree Regressor base regression model, an ensemble method using bagging is created which is nothing but the Random Forest Regressor. Instead of voting mechanism like in the case of Random Forest Classifier, we take the average of all the predictions by all the base estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reg_data = pd.read_csv(\"regression_housing.csv\")[[\"MSSubClass\", \"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"SalePrice\"]]\n",
    "X = reg_data.drop([\"SalePrice\"], axis=1).values\n",
    "y = reg_data[\"SalePrice\"].values\n",
    "from randomForest import randomForest\n",
    "rf = randomForest(type = \"regression\", max_depth=100, min_samples_leaf=1)\n",
    "rf.train(X, y)\n",
    "ypred = rf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49903.62236996378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "print(math.sqrt(mean_squared_error(ypred, y)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
